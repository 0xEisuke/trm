checkpoints_new\step_10000.ptのモデルサイズと生成結果に対する考察

いいログと図・生成結果がそろってるので、かなりはっきり考察できます。結論から言うと、**いまの学習はほぼ確実に「LMのラベルシフトが入っていない（=入力と同じトークンを当てる学習になっている）」系のバグ挙動**が出ています。なので **loss が異常に 0 付近まで落ちている**一方で、生成が **同語反復（レーベルレーベル…）**になります。

以下、指定の4点で整理します。

---

## 1) loss曲線（画像）の考察
checkpoints_new\training_curves.png
画像を見ると：

* **ce_loss（橙）が 35 → 8 くらいまで急落**（序盤）
* その後 **1桁台で少し横ばい**
* **~8k step 以降でさらに落ちて、~12k〜15k でほぼ 0 近辺へ**
* **aot_loss（緑）はほぼ0で、たまにスパイク**（副次タスクがあるなら正常っぽい）

### ここが決定的に不自然

自然言語LMで、**80万文級のコーパス**を **5.35Mパラメータ**で学習して、**クロスエントロピーがほぼ0**になるのは、普通は起きません（ほぼ「完全予測」＝データを暗記しても無理なレベル）。

この形の “異常に良すぎるloss” は典型的に：

* **ターゲットが1トークン右シフトされていない**（`labels == input_ids` のまま）
* つまり「次トークン予測」ではなく「**同一トークン当て**」になっている
  → causal mask でも各位置は **自分自身のトークンを見れる**ので、理論的に loss が 0 に近づけます

という症状に一致します。

---

## 2) モデルサイズ・構造の考察

### 構成

* vocab_size 16,000
* d_model 256 / n_heads 4
* max_seq_len 512
* latent_steps=2, deep_steps=2, supervision_steps=2
* dropout 0.1
* tie_embeddings True

### パラメータ

* 総 5.35M（かなり小さい）
* token_embed 4.10M
* output_head 4.10M
* tiny_net 1.12M

#### 注意（軽い違和感）

`tie_embeddings: True` なのに `token_embed` と `output_head` がそれぞれ 4.096M として両方カウントされてます。

* **実際に重み共有できているなら**、パラメータ数は「片方分」になるはず
* ただし、これは **count_params.py が“参照同一でも二重に数えてる”**だけの可能性もあります

👉 一度だけ確認するなら、PyTorchで
`model.token_embed.weight.data_ptr() == model.output_head.weight.data_ptr()`
みたいに **同一メモリか**見るのが確実です。

---

## 3) 出力結果の考察（生成）

入力：

> 昔々あるところに

出力：

> レーベルレーベルレーベル… 幕府幕府… 基礎基礎… コマ… 1937…（反復）

これは **「次トークン予測」になってないモデル**がよく出す挙動です。

もし学習が「各位置で *同じトークン* を当てる」になっていると、生成時には：

* 直前トークンを入れる
* モデルは「そのトークン自身」を最も高確率で返したがる
* 結果、**同じ単語を連打**しやすくなる

まさにこの症状です。

---

## 4) より良い出力を出すにはどうするべきか？

優先度順に “効く順” で。

### A. まず学習の正しさを直す（最重要）

**labels の1トークンシフト**が入っているか確認してください。

#### 正しい形（概念）

* 入力 `x = [t0, t1, t2, ...]`
* 教師 `y = [t1, t2, t3, ...]`（右に1つずらす）
* 最後の位置は予測不要なので `ignore_index=-100` などで無視

#### PyTorch例（典型）

```python
input_ids = ...
labels = input_ids.clone()
labels[:, :-1] = input_ids[:, 1:]
labels[:, -1] = -100  # ignore last
loss = F.cross_entropy(logits.view(-1, V), labels.view(-1), ignore_index=-100)
```

これを直すだけで、

* loss が 0 に張り付く現象が消える
* 生成が「同語反復」からかなり改善します

> もしすでに shift してるつもりなら、次に疑うのは
> **attention mask が“未来も見える”設定になっている**ことです。
> （それでも loss は異常に落ちます）

---

### B. validation loss をちゃんと見る

今の曲線が「train loss だけ」なら、**train のみで0**でも（暗記・バグ）見分けにくいです。

* valid の ce_loss / perplexity が自然に下がっているか
* train と valid の gap が異常に開いてないか

を見てください。

---

### C. 生成（デコード）側の改善（学習が正しくなった前提）

学習が直った後に、生成を安定させるコツ：

* temperature を下げる：`0.8 → 0.7 or 0.6`
* top_p を下げる：`0.9 → 0.85`
* top_k は 40 のままでOK
* 可能なら `repetition_penalty`（1.1〜1.2）や `no_repeat_ngram_size`（3〜4）を入れる

---

### D. モデル容量を少し増やす（必要なら）

5.35Mは「日本語の物語生成」にはだいぶ小さいので、正しく学習しても表現力に限界が来ます。
次の打ち手としては：

* d_model 256 → 384 or 512
* n_heads 4 → 6 or 8（d_modelと整合）
* max_seq_len 512 → 1024（データとVRAMが許せば）

ただし **A（ラベルシフト/マスク）を直すのが先**です。ここが壊れてると、容量を増やしても悪化します。

---

## 次にやるべき “最短の確認”

1つだけ確認質問を投げずに、あなたがすぐできるチェックを置きます：

* `train_trm_lm.py` の中で、**loss計算に使っている labels が input_ids と一致してないか**（=shiftされてるか）
* もし `labels = input_ids` のままなら、上のシフト処理を入れる

これを直した上で、**step 1000 くらい回した時点の生成**をもう一回見せてくれたら、次は

* 学習率/バッチ/seq_len/データ混合
* TRM特有の latent/deep の効き
  まで踏み込んで最適化案を出せます。
