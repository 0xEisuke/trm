# BOS (Beginning of Sentence) と EOS (End of Sentence) の扱いについて

本プロジェクトにおける特殊トークン（BOS, EOS）の扱いに関する設計思想と仕様をまとめます。

## 1. データ前処理 (`src/extract_texts.py`) における扱い

`src/extract_texts.py` は、SentencePieceトークナイザーの学習用コーパスを作成するためのスクリプトです。ここでは **BOS/EOS などの特殊トークンをテキストに含めません**。

### 理由
*   **トークナイザー学習の純粋性**: SentencePieceはテキスト内の文字の共起頻度からサブワード分割を学習します。ここに人為的な特殊トークン（`<s>`や`[BOS]`など）が文字列として混ざると、それらが「一般的な単語」の一部として誤って学習されてしまう恐れがあります。
*   **役割の分離**: 特殊トークンはトークナイザーの「機能（ID）」として定義すべきものであり、学習データの文字列自体に含めるべきではありません。

---

## 2. Dataset/Tokenizer (`src/dataset.py`) における扱い

`TokenizerWrapper` クラスのデフォルト設定は以下のようになっています。

*   `add_bos`: **False**
*   `add_eos`: **True**

### `add_bos=False` である理由（Trueでなくて良いのか？）

デフォルトで `False` となっていることには以下の正当性があり、**問題ありません**。

1.  **GPT系（Decoder-only）モデルの慣習**:
    *   GPT-2以降の多くの生成モデルでは、BOS（文頭トークン）を必須としません。
    *   モデルは「与えられたコンテキスト（トークン列）の続きを予測する」ように学習されます。もし学習データが常にBOSで始まっているとモデルが過学習し、「文の途中から生成を開始する」や「ユーザーのプロンプトに続きを書く」といったタスクにおいて、先頭にBOSがない場合に精度が落ちる可能性があります。
2.  **コンテキストの柔軟性と連続性**:
    *   長いテキストを学習する際、固定長のチャンク（例: 512トークン）で切り出して学習することがあります。この時、チャンクの先頭が必ずしも「文の始まり」とは限らず、前のチャンクからの続き（文の途中）である場合も多いです。
    *   このような場合に一律でBOSを付与すると、本来つながっている文脈として不自然なデータとなり、モデルの学習を阻害します。

**結論**: 任意のテキストの続きを生成する能力（Causal Language Modeling）を重視する場合、BOSを強制しない `False` 設定の方が一般的かつ安全です。

### `add_eos=True` である理由

一方で、EOS（文末トークン）は通常 **True**（付与する）とします。

1.  **文の区切りの学習**:
    *   モデルは「ここで文章が終わる」ということを明示的に学習する必要があります。
    *   これがないと、推論（生成）時に文章が終わるべきところでも停止できず、ダラダラと意味のない文字列を生成し続けてしまう問題が発生します。
2.  **データ境界の明示**:
    *   複数の短い文を結合して学習する場合や、バッチ処理でパディング（PAD）が入る場合、EOSがあることで「ここまでがひとつの有効な文である」という境界が明確になります。

---

## 3. まとめ

| トークン | 設定 | 理由 |
| :--- | :--- | :--- |
| **BOS** | `False` | 任意のコンテキストからの生成に対応するため、およびチャンク分割時の文脈維持のため、必須ではありません。 |
| **EOS** | `True` | 生成の終了タイミング（文の区切り）をモデルに学習させるため必須です。 |

現状の実装（`dataset.py` で `add_eos` のみ有効化して学習）は、生成モデル（Language Model）の標準的な構成として適切です。
